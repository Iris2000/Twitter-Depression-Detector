{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import pandas as pd\n",
    "import re \n",
    "import unidecode\n",
    "import string\n",
    "import itertools\n",
    "import contractions\n",
    "import liwc\n",
    "import gensim\n",
    "import gsdmm\n",
    "import pickle\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nrclex import NRCLex\n",
    "from collections import Counter\n",
    "from gsdmm import MovieGroupProcess\n",
    "from gensim.models.phrases import Phraser, Phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>conversation_id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>datetime</th>\n",
       "      <th>timezone</th>\n",
       "      <th>user_id</th>\n",
       "      <th>username</th>\n",
       "      <th>name</th>\n",
       "      <th>place</th>\n",
       "      <th>tweet</th>\n",
       "      <th>...</th>\n",
       "      <th>source</th>\n",
       "      <th>user_rt_id</th>\n",
       "      <th>user_rt</th>\n",
       "      <th>retweet_id</th>\n",
       "      <th>reply_to</th>\n",
       "      <th>retweet_date</th>\n",
       "      <th>translate</th>\n",
       "      <th>trans_src</th>\n",
       "      <th>trans_dest</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.470000e+18</td>\n",
       "      <td>1.470000e+18</td>\n",
       "      <td>2021-12-16 14:27:14 Malay Peninsula Standard Time</td>\n",
       "      <td>16/12/2021 14:27</td>\n",
       "      <td>800</td>\n",
       "      <td>2.676289e+09</td>\n",
       "      <td>robertevans97</td>\n",
       "      <td>Robert Evans</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@arielhelwani @DustinPoirier @NateDiaz209 Big ...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'screen_name': 'arielhelwani', 'name': 'Arie...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.470000e+18</td>\n",
       "      <td>1.470000e+18</td>\n",
       "      <td>2021-12-16 14:27:14 Malay Peninsula Standard Time</td>\n",
       "      <td>16/12/2021 14:27</td>\n",
       "      <td>800</td>\n",
       "      <td>7.190000e+17</td>\n",
       "      <td>flintmorris</td>\n",
       "      <td>FlintMorris</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@ItsRyanGonzales @100Thieves @RabidDoh @thundo...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'screen_name': 'ItsRyanGonzales', 'name': '1...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.470000e+18</td>\n",
       "      <td>1.470000e+18</td>\n",
       "      <td>2021-12-16 14:27:14 Malay Peninsula Standard Time</td>\n",
       "      <td>16/12/2021 14:27</td>\n",
       "      <td>800</td>\n",
       "      <td>1.370000e+18</td>\n",
       "      <td>chardinite</td>\n",
       "      <td>Chardinite</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@Tinkzorg Gen Z can't wield the mandate of hea...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'screen_name': 'Tinkzorg', 'name': 'Miriam G...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.470000e+18</td>\n",
       "      <td>1.470000e+18</td>\n",
       "      <td>2021-12-16 14:27:14 Malay Peninsula Standard Time</td>\n",
       "      <td>16/12/2021 14:27</td>\n",
       "      <td>800</td>\n",
       "      <td>1.518807e+08</td>\n",
       "      <td>ghobubo</td>\n",
       "      <td>Blixy Scrumpfer</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@JortsTheCat has his own account and 20K follo...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.470000e+18</td>\n",
       "      <td>1.470000e+18</td>\n",
       "      <td>2021-12-16 14:27:14 Malay Peninsula Standard Time</td>\n",
       "      <td>16/12/2021 14:27</td>\n",
       "      <td>800</td>\n",
       "      <td>1.430000e+18</td>\n",
       "      <td>mdarahimkhan2</td>\n",
       "      <td>moni</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@BitKeepOS @NEARProtocol This is a really grea...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'screen_name': 'BitKeepOS', 'name': 'BitKeep...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             id  conversation_id  \\\n",
       "0  1.470000e+18     1.470000e+18   \n",
       "1  1.470000e+18     1.470000e+18   \n",
       "2  1.470000e+18     1.470000e+18   \n",
       "3  1.470000e+18     1.470000e+18   \n",
       "4  1.470000e+18     1.470000e+18   \n",
       "\n",
       "                                          created_at          datetime  \\\n",
       "0  2021-12-16 14:27:14 Malay Peninsula Standard Time  16/12/2021 14:27   \n",
       "1  2021-12-16 14:27:14 Malay Peninsula Standard Time  16/12/2021 14:27   \n",
       "2  2021-12-16 14:27:14 Malay Peninsula Standard Time  16/12/2021 14:27   \n",
       "3  2021-12-16 14:27:14 Malay Peninsula Standard Time  16/12/2021 14:27   \n",
       "4  2021-12-16 14:27:14 Malay Peninsula Standard Time  16/12/2021 14:27   \n",
       "\n",
       "   timezone       user_id       username             name place  \\\n",
       "0       800  2.676289e+09  robertevans97     Robert Evans   NaN   \n",
       "1       800  7.190000e+17    flintmorris      FlintMorris   NaN   \n",
       "2       800  1.370000e+18     chardinite       Chardinite   NaN   \n",
       "3       800  1.518807e+08        ghobubo  Blixy Scrumpfer   NaN   \n",
       "4       800  1.430000e+18  mdarahimkhan2             moni   NaN   \n",
       "\n",
       "                                               tweet  ... source user_rt_id  \\\n",
       "0  @arielhelwani @DustinPoirier @NateDiaz209 Big ...  ...    NaN        NaN   \n",
       "1  @ItsRyanGonzales @100Thieves @RabidDoh @thundo...  ...    NaN        NaN   \n",
       "2  @Tinkzorg Gen Z can't wield the mandate of hea...  ...    NaN        NaN   \n",
       "3  @JortsTheCat has his own account and 20K follo...  ...    NaN        NaN   \n",
       "4  @BitKeepOS @NEARProtocol This is a really grea...  ...    NaN        NaN   \n",
       "\n",
       "  user_rt retweet_id                                           reply_to  \\\n",
       "0     NaN        NaN  [{'screen_name': 'arielhelwani', 'name': 'Arie...   \n",
       "1     NaN        NaN  [{'screen_name': 'ItsRyanGonzales', 'name': '1...   \n",
       "2     NaN        NaN  [{'screen_name': 'Tinkzorg', 'name': 'Miriam G...   \n",
       "3     NaN        NaN                                                 []   \n",
       "4     NaN        NaN  [{'screen_name': 'BitKeepOS', 'name': 'BitKeep...   \n",
       "\n",
       "   retweet_date  translate trans_src trans_dest  target  \n",
       "0           NaN        NaN       NaN        NaN  normal  \n",
       "1           NaN        NaN       NaN        NaN  normal  \n",
       "2           NaN        NaN       NaN        NaN  normal  \n",
       "3           NaN        NaN       NaN        NaN  normal  \n",
       "4           NaN        NaN       NaN        NaN  normal  \n",
       "\n",
       "[5 rows x 36 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import normal csv\n",
    "normal_df = pd.read_csv (r'C://Users//lvlip//Documents//BCSI Sem 6//FYP 4202//CSV//normal_twint//NormalCombine.csv', engine='python')\n",
    "normal_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>conversation_id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>datetime</th>\n",
       "      <th>timezone</th>\n",
       "      <th>user_id</th>\n",
       "      <th>username</th>\n",
       "      <th>name</th>\n",
       "      <th>place</th>\n",
       "      <th>tweet</th>\n",
       "      <th>...</th>\n",
       "      <th>source</th>\n",
       "      <th>user_rt_id</th>\n",
       "      <th>user_rt</th>\n",
       "      <th>retweet_id</th>\n",
       "      <th>reply_to</th>\n",
       "      <th>retweet_date</th>\n",
       "      <th>translate</th>\n",
       "      <th>trans_src</th>\n",
       "      <th>trans_dest</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.470000e+18</td>\n",
       "      <td>1.470000e+18</td>\n",
       "      <td>2021-12-23 06:47:54 Malay Peninsula Standard Time</td>\n",
       "      <td>23/12/2021 6:47</td>\n",
       "      <td>800</td>\n",
       "      <td>361381402.0</td>\n",
       "      <td>mike_raines48</td>\n",
       "      <td>Michael</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Iâ€™m just having the worst week like nothing go...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>depressed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.470000e+18</td>\n",
       "      <td>1.470000e+18</td>\n",
       "      <td>2021-12-22 06:11:57 Malay Peninsula Standard Time</td>\n",
       "      <td>22/12/2021 6:11</td>\n",
       "      <td>800</td>\n",
       "      <td>361381402.0</td>\n",
       "      <td>mike_raines48</td>\n",
       "      <td>Michael</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The unidentifed neurodivergent urge to figure ...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>depressed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.470000e+18</td>\n",
       "      <td>1.470000e+18</td>\n",
       "      <td>2021-12-20 07:39:55 Malay Peninsula Standard Time</td>\n",
       "      <td>20/12/2021 7:39</td>\n",
       "      <td>800</td>\n",
       "      <td>361381402.0</td>\n",
       "      <td>mike_raines48</td>\n",
       "      <td>Michael</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Hate when mfs lie to my face when I already kn...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>depressed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.470000e+18</td>\n",
       "      <td>1.470000e+18</td>\n",
       "      <td>2021-12-19 22:13:35 Malay Peninsula Standard Time</td>\n",
       "      <td>19/12/2021 22:13</td>\n",
       "      <td>800</td>\n",
       "      <td>361381402.0</td>\n",
       "      <td>mike_raines48</td>\n",
       "      <td>Michael</td>\n",
       "      <td>NaN</td>\n",
       "      <td>no one cares enough about me to take the initi...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>depressed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.470000e+18</td>\n",
       "      <td>1.470000e+18</td>\n",
       "      <td>2021-12-18 12:39:55 Malay Peninsula Standard Time</td>\n",
       "      <td>18/12/2021 12:39</td>\n",
       "      <td>800</td>\n",
       "      <td>361381402.0</td>\n",
       "      <td>mike_raines48</td>\n",
       "      <td>Michael</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I just want somebody to vibe with , go out, an...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>depressed</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             id  conversation_id  \\\n",
       "0  1.470000e+18     1.470000e+18   \n",
       "1  1.470000e+18     1.470000e+18   \n",
       "2  1.470000e+18     1.470000e+18   \n",
       "3  1.470000e+18     1.470000e+18   \n",
       "4  1.470000e+18     1.470000e+18   \n",
       "\n",
       "                                          created_at          datetime  \\\n",
       "0  2021-12-23 06:47:54 Malay Peninsula Standard Time   23/12/2021 6:47   \n",
       "1  2021-12-22 06:11:57 Malay Peninsula Standard Time   22/12/2021 6:11   \n",
       "2  2021-12-20 07:39:55 Malay Peninsula Standard Time   20/12/2021 7:39   \n",
       "3  2021-12-19 22:13:35 Malay Peninsula Standard Time  19/12/2021 22:13   \n",
       "4  2021-12-18 12:39:55 Malay Peninsula Standard Time  18/12/2021 12:39   \n",
       "\n",
       "  timezone      user_id       username     name  place  \\\n",
       "0      800  361381402.0  mike_raines48  Michael    NaN   \n",
       "1      800  361381402.0  mike_raines48  Michael    NaN   \n",
       "2      800  361381402.0  mike_raines48  Michael    NaN   \n",
       "3      800  361381402.0  mike_raines48  Michael    NaN   \n",
       "4      800  361381402.0  mike_raines48  Michael    NaN   \n",
       "\n",
       "                                               tweet  ... source user_rt_id  \\\n",
       "0  Iâ€™m just having the worst week like nothing go...  ...    NaN        NaN   \n",
       "1  The unidentifed neurodivergent urge to figure ...  ...    NaN        NaN   \n",
       "2  Hate when mfs lie to my face when I already kn...  ...    NaN        NaN   \n",
       "3  no one cares enough about me to take the initi...  ...    NaN        NaN   \n",
       "4  I just want somebody to vibe with , go out, an...  ...    NaN        NaN   \n",
       "\n",
       "  user_rt retweet_id  reply_to  retweet_date  translate trans_src trans_dest  \\\n",
       "0     NaN        NaN        []           NaN        NaN       NaN        NaN   \n",
       "1     NaN        NaN        []           NaN        NaN       NaN        NaN   \n",
       "2     NaN        NaN        []           NaN        NaN       NaN        NaN   \n",
       "3     NaN        NaN        []           NaN        NaN       NaN        NaN   \n",
       "4     NaN        NaN        []           NaN        NaN       NaN        NaN   \n",
       "\n",
       "      target  \n",
       "0  depressed  \n",
       "1  depressed  \n",
       "2  depressed  \n",
       "3  depressed  \n",
       "4  depressed  \n",
       "\n",
       "[5 rows x 36 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import depressed csv\n",
    "depressed_df = pd.read_csv (r'C://Users//lvlip//Documents//BCSI Sem 6//FYP 4202//CSV//depression_twint//DepressedCombine.csv', engine='python')\n",
    "depressed_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10746, 36)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# combine two dataframe\n",
    "df = pd.concat([normal_df, depressed_df], axis=0)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 10746 entries, 0 to 5551\n",
      "Data columns (total 36 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   id               10746 non-null  float64\n",
      " 1   conversation_id  10746 non-null  float64\n",
      " 2   created_at       10746 non-null  object \n",
      " 3   datetime         10746 non-null  object \n",
      " 4   timezone         10746 non-null  object \n",
      " 5   user_id          10746 non-null  float64\n",
      " 6   username         10746 non-null  object \n",
      " 7   name             10746 non-null  object \n",
      " 8   place            1 non-null      object \n",
      " 9   tweet            10746 non-null  object \n",
      " 10  language         10745 non-null  object \n",
      " 11  mentions         10745 non-null  object \n",
      " 12  urls             10745 non-null  object \n",
      " 13  photos           10745 non-null  object \n",
      " 14  replies_count    10745 non-null  float64\n",
      " 15  retweets_count   10745 non-null  float64\n",
      " 16  likes_count      10745 non-null  float64\n",
      " 17  hashtags         10745 non-null  object \n",
      " 18  cashtags         10745 non-null  object \n",
      " 19  link             10745 non-null  object \n",
      " 20  retweet          10745 non-null  object \n",
      " 21  quote_url        610 non-null    object \n",
      " 22  video            10745 non-null  float64\n",
      " 23  thumbnail        770 non-null    object \n",
      " 24  near             0 non-null      float64\n",
      " 25  geo              0 non-null      float64\n",
      " 26  source           0 non-null      float64\n",
      " 27  user_rt_id       0 non-null      float64\n",
      " 28  user_rt          0 non-null      float64\n",
      " 29  retweet_id       0 non-null      float64\n",
      " 30  reply_to         10745 non-null  object \n",
      " 31  retweet_date     0 non-null      float64\n",
      " 32  translate        0 non-null      float64\n",
      " 33  trans_src        0 non-null      float64\n",
      " 34  trans_dest       0 non-null      float64\n",
      " 35  target           10746 non-null  object \n",
      "dtypes: float64(17), object(19)\n",
      "memory usage: 3.0+ MB\n"
     ]
    }
   ],
   "source": [
    "# check the info\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract required columns\n",
    "def extract(df):\n",
    "    df = df[['username', 'datetime', 'tweet', 'target']]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = extract(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "username    0\n",
       "datetime    0\n",
       "tweet       0\n",
       "target      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the total number of null value\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>username</th>\n",
       "      <th>datetime</th>\n",
       "      <th>tweet</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1617</th>\n",
       "      <td>vantksooo</td>\n",
       "      <td>16/12/21 14:26:46</td>\n",
       "      <td>if u can see this tweet, im encouraging u to v...</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2169</th>\n",
       "      <td>themosthi904</td>\n",
       "      <td>17/12/21 16:21:55</td>\n",
       "      <td>??</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3062</th>\n",
       "      <td>crazyotters11</td>\n",
       "      <td>17/12/21 16:21:42</td>\n",
       "      <td>????</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>toogay_tootired</td>\n",
       "      <td>17/12/2021 16:21</td>\n",
       "      <td>I feel so sick itâ€™s awful. My Drs apparently c...</td>\n",
       "      <td>depressed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1699</th>\n",
       "      <td>fashionkattz</td>\n",
       "      <td>20/12/2021 15:18</td>\n",
       "      <td>And just like that im #depressed tired of life...</td>\n",
       "      <td>depressed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5412</th>\n",
       "      <td>im_shy_guy</td>\n",
       "      <td>9/12/2021 4:56</td>\n",
       "      <td>Nobody cares until its too late... No they don...</td>\n",
       "      <td>depressed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5522</th>\n",
       "      <td>shoppingblonde</td>\n",
       "      <td>23/12/2021 12:16</td>\n",
       "      <td>#depression Sucks! I am always so hard on myse...</td>\n",
       "      <td>depressed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5538</th>\n",
       "      <td>lunarmeow</td>\n",
       "      <td>19/12/2021 14:10</td>\n",
       "      <td>I can't stand feeling. Where's the numb I need...</td>\n",
       "      <td>depressed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5541</th>\n",
       "      <td>bo00mer</td>\n",
       "      <td>19/12/2021 3:10</td>\n",
       "      <td>@SickNotWeak @heylandsberg Bad right now, havi...</td>\n",
       "      <td>depressed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5544</th>\n",
       "      <td>bo00mer</td>\n",
       "      <td>18/12/2021 7:41</td>\n",
       "      <td>@SickNotWeak @heylandsberg I had an extreme ba...</td>\n",
       "      <td>depressed</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>68 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             username           datetime  \\\n",
       "1617        vantksooo  16/12/21 14:26:46   \n",
       "2169     themosthi904  17/12/21 16:21:55   \n",
       "3062    crazyotters11  17/12/21 16:21:42   \n",
       "20    toogay_tootired   17/12/2021 16:21   \n",
       "1699     fashionkattz   20/12/2021 15:18   \n",
       "...               ...                ...   \n",
       "5412       im_shy_guy     9/12/2021 4:56   \n",
       "5522   shoppingblonde   23/12/2021 12:16   \n",
       "5538        lunarmeow   19/12/2021 14:10   \n",
       "5541          bo00mer    19/12/2021 3:10   \n",
       "5544          bo00mer    18/12/2021 7:41   \n",
       "\n",
       "                                                  tweet     target  \n",
       "1617  if u can see this tweet, im encouraging u to v...     normal  \n",
       "2169                                                 ??     normal  \n",
       "3062                                               ????     normal  \n",
       "20    I feel so sick itâ€™s awful. My Drs apparently c...  depressed  \n",
       "1699  And just like that im #depressed tired of life...  depressed  \n",
       "...                                                 ...        ...  \n",
       "5412  Nobody cares until its too late... No they don...  depressed  \n",
       "5522  #depression Sucks! I am always so hard on myse...  depressed  \n",
       "5538  I can't stand feeling. Where's the numb I need...  depressed  \n",
       "5541  @SickNotWeak @heylandsberg Bad right now, havi...  depressed  \n",
       "5544  @SickNotWeak @heylandsberg I had an extreme ba...  depressed  \n",
       "\n",
       "[68 rows x 4 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check duplicate tweets\n",
    "df[df.duplicated('tweet')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicate tweets\n",
    "def duplicate(df):\n",
    "    # drop duplicate tweets\n",
    "    df = df.drop_duplicates(subset=['tweet'])\n",
    "    df = df.reset_index(drop=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = duplicate(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove non-english character\n",
    "def decode(df):\n",
    "    decode_data = []\n",
    "    encoded_data = [x.encode('ascii', 'ignore') for x in df['tweet']]\n",
    "    for data in encoded_data:\n",
    "        decode_data.append(data.decode())\n",
    "    df['tweet'] = decode_data\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = decode(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a tweet list from dataframe for cleaning\n",
    "def tweet_list(df):\n",
    "    tweet = df['tweet'].iloc[:]\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = tweet_list(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text cleaning\n",
    "def clean(tweet):\n",
    "    # remove links\n",
    "    tweet = [re.sub(r'https?://\\S+|www\\.\\S+', '', x) for x in tweet]\n",
    "    # remove emails\n",
    "    tweet = [re.sub(r'\\S*@\\S*\\s?', '', x) for x in tweet]\n",
    "    # remove mentions\n",
    "    tweet = [re.sub(r'@', '', x) for x in tweet]\n",
    "    # remove hashtags\n",
    "    tweet = [re.sub(r'#[A-Za-z0-9_]+', '', x) for x in tweet]\n",
    "    # remove unicode, change quotation marks, â„¢ to (tm), and so on\n",
    "    tweet = [unidecode.unidecode(x) for x in tweet]\n",
    "    # remove specific character / word\n",
    "    tweet = [re.sub(r'&amp;', '', x) for x in tweet]\n",
    "    tweet = [re.sub(r'\"', '', x) for x in tweet]\n",
    "    tweet = [re.sub(r'\\(tm\\)','', x) for x in tweet]\n",
    "    tweet = [re.sub(r'\\(c\\)','', x) for x in tweet]\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = clean(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert emojis to text\n",
    "def emojis(tweet):\n",
    "    # encode html entities - some of them are emojis\n",
    "    tweet = [re.sub(\"&gt;\",\">\", x) for x in tweet]\n",
    "    tweet = [re.sub(\"&lt;\",\"<\", x) for x in tweet]\n",
    "    # emojis to text\n",
    "    tweet = [re.sub(\";\\)\",\"wink\", x) for x in tweet]\n",
    "    tweet = [re.sub(\";-\\)\",\"wink\", x) for x in tweet]\n",
    "    tweet = [re.sub(\";D\",\"wink\", x) for x in tweet]\n",
    "    tweet = [re.sub(\":\\)\",\"smiley\", x) for x in tweet]\n",
    "    tweet = [re.sub(\":>\",\"smiley\", x) for x in tweet]\n",
    "    tweet = [re.sub(\":-\\)\",\"smiley\", x) for x in tweet]\n",
    "    tweet = [re.sub(\":^\\)\",\"smiley\", x) for x in tweet]\n",
    "    tweet = [re.sub(\":]\",\"smiley\", x) for x in tweet]\n",
    "    tweet = [re.sub(\"=\\)\",\"smiley\", x) for x in tweet]\n",
    "    tweet = [re.sub(\":'\\)\",\"happy\", x) for x in tweet]\n",
    "    tweet = [re.sub(\">v<\",\"happy\", x) for x in tweet]\n",
    "    tweet = [re.sub(\":D\",\"grinning\", x) for x in tweet]\n",
    "    tweet = [re.sub(\"\\^\\^\",\"joy\", x) for x in tweet]\n",
    "    tweet = [re.sub(\"\\^__\\^\",\"joy\", x) for x in tweet]\n",
    "    tweet = [re.sub(\"xD\",\"laughing\", x) for x in tweet]\n",
    "    tweet = [re.sub(\"XD\",\"laughing\", x) for x in tweet]\n",
    "    tweet = [re.sub(\":p\",\"cheeky\", x) for x in tweet]\n",
    "    tweet = [re.sub(\":/\",\"hesitant\", x) for x in tweet]\n",
    "    tweet = [re.sub(\"<3\",\"love\", x) for x in tweet]\n",
    "    tweet = [re.sub(\">_>\",\"devious\", x) for x in tweet]\n",
    "    tweet = [re.sub(\">.>\",\"devious\", x) for x in tweet]\n",
    "    tweet = [re.sub(\":\\(\",\"sad\", x) for x in tweet]\n",
    "    tweet = [re.sub(\"T__T\",\"crying\", x) for x in tweet]\n",
    "    tweet = [re.sub(\";-;\",\"crying\", x) for x in tweet]\n",
    "    tweet = [re.sub(\"_\\(:3\",\"tired\", x) for x in tweet]\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = emojis(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert text to lowercase        \n",
    "def lowercase(tweet):\n",
    "    tweet = [x.lower() for x in tweet]\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = lowercase(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expand the contractions\n",
    "def contracts(tweet):\n",
    "    tweet = [contractions.fix(x, slang=True) for x in tweet]\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = contracts(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove punctuations\n",
    "def punctuations(tweet):\n",
    "    for character in string.punctuation:\n",
    "        tweet = [x.replace(character, ' ') for x in tweet]\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = punctuations(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix word lengthening\n",
    "def lengthening(tweet):\n",
    "    tweet = [''.join(''.join(s)[:2] for _, s in itertools.groupby(x)) for x in tweet]\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = lengthening(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expand jargons\n",
    "def jargons(tweet):\n",
    "    tweet = [re.sub(r'\\bppl\\b','people', x) for x in tweet]\n",
    "    tweet = [re.sub(r'\\brn\\b','right now', x) for x in tweet]\n",
    "    tweet = [re.sub(r'\\bw\\b','with', x) for x in tweet]\n",
    "    tweet = [re.sub(r'\\br\\b','are', x) for x in tweet]\n",
    "\n",
    "    jargons = {\n",
    "    'asap': 'as fast as possible',\n",
    "    'brb': 'be right back',\n",
    "    'lol': 'laugh',\n",
    "    'lolz': 'laugh',\n",
    "    'lmao': 'laugh',\n",
    "    'lmaoo': 'laugh',\n",
    "    'tbh': 'to be honest',\n",
    "    'abt': 'about',\n",
    "    'smh': 'somehow',\n",
    "    'btw': 'by the way',\n",
    "    'gatchu': 'got you',\n",
    "    'atm': 'at the moment',\n",
    "    'txt': 'text',\n",
    "    'yrs': 'years',\n",
    "    'fyi': 'for your information',\n",
    "    'gna': 'gonna',\n",
    "    'gn': 'good night',\n",
    "    'yt': 'youtube',\n",
    "    'vid': 'video',\n",
    "    'vids': 'videos',\n",
    "    'pic': 'picture',\n",
    "    'thru': 'through',\n",
    "    'tho': 'though',\n",
    "    'imma': 'i am going to',\n",
    "    'bff': 'best friend forever',\n",
    "    'dm': 'direct message',\n",
    "    'cmon': 'come on',\n",
    "    'bcz': 'because',\n",
    "    'bc': 'because',\n",
    "    'rly': 'really',\n",
    "    'idk': 'i do not know',\n",
    "    'ikr': 'i know right',\n",
    "    'plz': 'please',\n",
    "    'pls': 'please',\n",
    "    'bro': 'brother',\n",
    "    'bruh': 'brother',\n",
    "    'dunno': 'do not know'\n",
    "    }\n",
    "\n",
    "    for i in range(len(tweet)):\n",
    "        for word in tweet[i].split():\n",
    "            if word in jargons:\n",
    "                tweet[i] = tweet[i].replace(word, jargons[word])\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = jargons(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove word with numbers -> such as datetime\n",
    "def numbers(tweet):\n",
    "    tweet = [re.sub(r'\\w+\\d\\w+', '', x) for x in tweet]\n",
    "    tweet = [re.sub(r'\\d+', '', x) for x in tweet]\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = numbers(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change 's -> is, delete s as some become s only\n",
    "def replace(tweet):\n",
    "    tweet = [re.sub(r'\\bs\\b','', x) for x in tweet]\n",
    "    tweet = [re.sub(r'\\bid\\b','i had', x) for x in tweet]\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = replace(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove whitespace\n",
    "def whitespace(tweet):\n",
    "    tweet = [re.sub(' +',' ', x) for x in tweet]\n",
    "    tweet = [x.rstrip().lstrip() for x in tweet]\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = whitespace(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract pos tag\n",
    "pos_tagged = [nltk.pos_tag(nltk.word_tokenize(x)) for x in tweet]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WordNetLemmatizer with appropriate pos tags\n",
    "def pos_tagger(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:         \n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace the original pos tag with simpler naming\n",
    "wordnet_tagged = [list(map(lambda x: (x[0], pos_tagger(x[1])), x)) for x in pos_tagged]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatize tweets with the pos tag created\n",
    "def lemmatize(wordnet_tagged):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tweet = []\n",
    "\n",
    "    for i in range (len(wordnet_tagged)):\n",
    "        lemmatized_sentence = []\n",
    "        for word, tag in wordnet_tagged[i]:\n",
    "            if tag is None:\n",
    "                # if there is no available tag, append the token without lemmatize it\n",
    "                lemmatized_sentence.append(word)\n",
    "            else:       \n",
    "                # use the tag to lemmatize the word\n",
    "                lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))\n",
    "        lemmatized_join = ' '.join(lemmatized_sentence)\n",
    "        lemmatized_tweet.append(lemmatized_join)\n",
    "    return lemmatized_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = lemmatize(wordnet_tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stopwords\n",
    "def remove_stopwords(tweet):\n",
    "    # reserve 1st 2nd 3rd person pronoun and \"no\"\n",
    "    stop_words = stopwords.words('english')\n",
    "    stop_words = ['what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are','was', 'were', \n",
    "                  'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', \n",
    "                  'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', \n",
    "                  'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', \n",
    "                  'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', \n",
    "                  'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', \n",
    "                  'other', 'some', 'such', 'nor', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', \n",
    "                  'will', 'just', 'should', 'now', 'd', 'll', 'm', 'o', 're', 've', 'y',\n",
    "                 ]\n",
    "\n",
    "    text_tokens = [word_tokenize(x) for x in tweet]\n",
    "    \n",
    "    combine_without_sw = []\n",
    "    combine_text = []\n",
    "    \n",
    "    for text in text_tokens:\n",
    "        text_without_sw = [word for word in text if word not in stop_words]\n",
    "        combine_without_sw.append(text_without_sw)\n",
    "    \n",
    "    for word in combine_without_sw:\n",
    "        combine_join = ' '.join(word)\n",
    "        combine_text.append(combine_join)\n",
    "    return combine_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = remove_stopwords(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = lowercase(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine tweet and df\n",
    "def combine(tweet, df):\n",
    "    # convert tweet list to dataframe\n",
    "    cleaned_tweet = pd.DataFrame(tweet, columns=['cleaned_tweet'])\n",
    "    # concat df with cleaned_tweet\n",
    "    df = pd.concat([df, cleaned_tweet], axis=1)\n",
    "    # reordering the columns\n",
    "    df = df[['username', 'datetime', 'tweet', 'cleaned_tweet', 'target']]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = combine(tweet, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove tweets with <= 1 words \n",
    "def less_words(df):\n",
    "    # check the tweets with <= words\n",
    "    count = 0\n",
    "    delete = []\n",
    "\n",
    "    for i in range (len(df)):\n",
    "        if len(df['cleaned_tweet'][i].split()) <= 1:\n",
    "            print(df['cleaned_tweet'][i])\n",
    "            delete.append(i)\n",
    "            count += 1\n",
    "\n",
    "    print(count)\n",
    "    print(delete)\n",
    "    \n",
    "    # remove empty elements\n",
    "    df = df.drop(df.index[delete])\n",
    "    # reset index\n",
    "    df = df.reset_index(drop=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "come\n",
      "honour\n",
      "question\n",
      "cop\n",
      "member\n",
      "beautiful\n",
      "truth\n",
      "moon\n",
      "bronze\n",
      "\n",
      "win\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "c\n",
      "\n",
      "\n",
      "best\n",
      "definitely\n",
      "g\n",
      "thankful\n",
      "\n",
      "end\n",
      "literally\n",
      "fk\n",
      "yahoo\n",
      "\n",
      "\n",
      "ww\n",
      "\n",
      "them\n",
      "\n",
      "hopefully\n",
      "fuck\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "left\n",
      "\n",
      "\n",
      "\n",
      "disrespect\n",
      "\n",
      "\n",
      "eve\n",
      "\n",
      "wallpaper\n",
      "sea\n",
      "chevalet\n",
      "\n",
      "accurate\n",
      "sure\n",
      "mine\n",
      "\n",
      "\n",
      "good\n",
      "\n",
      "\n",
      "\n",
      "moon\n",
      "\n",
      "\n",
      "buy\n",
      "selector\n",
      "awesome\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "kid\n",
      "hahahaha\n",
      "yes\n",
      "durrhh\n",
      "\n",
      "proof\n",
      "\n",
      "first\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "yes\n",
      "sweet\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "baby\n",
      "\n",
      "fuck\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "next\n",
      "\n",
      "\n",
      "\n",
      "sad\n",
      "literally\n",
      "realization\n",
      "glass\n",
      "\n",
      "\n",
      "\n",
      "leo\n",
      "\n",
      "\n",
      "\n",
      "confess\n",
      "you\n",
      "\n",
      "profit\n",
      "show\n",
      "anything\n",
      "photographer\n",
      "spirit\n",
      "floor\n",
      "life\n",
      "you\n",
      "go\n",
      "option\n",
      "hell\n",
      "exhaust\n",
      "131\n",
      "[236, 399, 471, 576, 599, 688, 1242, 1262, 1326, 1666, 1670, 1685, 1686, 1732, 1740, 1773, 1787, 1790, 1808, 1833, 1853, 1869, 1879, 1890, 1901, 1923, 1934, 1946, 1955, 1959, 2007, 2043, 2051, 2053, 2054, 2082, 2101, 2119, 2126, 2129, 2130, 2135, 2170, 2172, 2177, 2228, 2243, 2259, 2264, 2277, 2286, 2310, 2322, 2339, 2377, 2380, 2382, 2402, 2404, 2446, 2466, 2475, 2488, 2505, 2511, 2531, 2545, 2546, 2554, 2581, 2584, 2606, 2622, 2625, 2628, 2636, 2637, 2643, 2657, 2680, 2708, 2718, 2747, 2763, 2782, 2784, 2795, 2802, 2842, 2848, 2874, 2895, 2946, 2956, 2957, 2962, 2967, 2977, 3033, 3034, 3065, 3075, 3093, 3099, 3113, 3115, 3120, 3124, 3136, 3141, 3156, 3193, 3214, 3223, 3225, 3286, 3308, 3439, 3492, 3704, 4134, 4148, 4397, 4611, 4671, 4737, 4838, 4841, 4884, 7955, 8662]\n"
     ]
    }
   ],
   "source": [
    "df = less_words(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get sentiment polarity score with VADER\n",
    "def vader(df):\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    df['tweet_compound'] = df['tweet'].apply(lambda x:analyzer.polarity_scores(x)['compound'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = vader(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get emotion lexicon with nrclex\n",
    "def nrclex(df):\n",
    "    # all emotions affect frequencies in one column\n",
    "    df['emotions'] = df['cleaned_tweet'].apply(lambda x: NRCLex(x).affect_frequencies)\n",
    "    # split emotions into respective columns\n",
    "    df = pd.concat([df.drop(['emotions'], axis=1), df['emotions'].apply(pd.Series)], axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = nrclex(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>username</th>\n",
       "      <th>datetime</th>\n",
       "      <th>tweet</th>\n",
       "      <th>cleaned_tweet</th>\n",
       "      <th>target</th>\n",
       "      <th>tweet_compound</th>\n",
       "      <th>fear</th>\n",
       "      <th>anger</th>\n",
       "      <th>anticipation</th>\n",
       "      <th>trust</th>\n",
       "      <th>surprise</th>\n",
       "      <th>positive</th>\n",
       "      <th>negative</th>\n",
       "      <th>sadness</th>\n",
       "      <th>disgust</th>\n",
       "      <th>joy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>robertevans97</td>\n",
       "      <td>16/12/21 14:27:14</td>\n",
       "      <td>@arielhelwani @DustinPoirier @NateDiaz209 Big ...</td>\n",
       "      <td>big fight dustin want prove point nate</td>\n",
       "      <td>normal</td>\n",
       "      <td>-0.3182</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>flintmorris</td>\n",
       "      <td>16/12/21 14:27:14</td>\n",
       "      <td>@ItsRyanGonzales @100Thieves @RabidDoh @thundo...</td>\n",
       "      <td>they choose one best naruto opening song big</td>\n",
       "      <td>normal</td>\n",
       "      <td>0.6369</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>chardinite</td>\n",
       "      <td>16/12/21 14:27:14</td>\n",
       "      <td>@Tinkzorg Gen Z can't wield the mandate of hea...</td>\n",
       "      <td>gen z not wield mandate heaven confirm</td>\n",
       "      <td>normal</td>\n",
       "      <td>0.5106</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ghobubo</td>\n",
       "      <td>16/12/21 14:27:14</td>\n",
       "      <td>@JortsTheCat has his own account and 20K follo...</td>\n",
       "      <td>his account follower sweet potato</td>\n",
       "      <td>normal</td>\n",
       "      <td>0.4588</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mdarahimkhan2</td>\n",
       "      <td>16/12/21 14:27:14</td>\n",
       "      <td>@BitKeepOS @NEARProtocol This is a really grea...</td>\n",
       "      <td>really great excellent project thank you oppor...</td>\n",
       "      <td>normal</td>\n",
       "      <td>0.9604</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.187500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        username           datetime  \\\n",
       "0  robertevans97  16/12/21 14:27:14   \n",
       "1    flintmorris  16/12/21 14:27:14   \n",
       "2     chardinite  16/12/21 14:27:14   \n",
       "3        ghobubo  16/12/21 14:27:14   \n",
       "4  mdarahimkhan2  16/12/21 14:27:14   \n",
       "\n",
       "                                               tweet  \\\n",
       "0  @arielhelwani @DustinPoirier @NateDiaz209 Big ...   \n",
       "1  @ItsRyanGonzales @100Thieves @RabidDoh @thundo...   \n",
       "2  @Tinkzorg Gen Z can't wield the mandate of hea...   \n",
       "3  @JortsTheCat has his own account and 20K follo...   \n",
       "4  @BitKeepOS @NEARProtocol This is a really grea...   \n",
       "\n",
       "                                       cleaned_tweet  target  tweet_compound  \\\n",
       "0             big fight dustin want prove point nate  normal         -0.3182   \n",
       "1       they choose one best naruto opening song big  normal          0.6369   \n",
       "2             gen z not wield mandate heaven confirm  normal          0.5106   \n",
       "3                  his account follower sweet potato  normal          0.4588   \n",
       "4  really great excellent project thank you oppor...  normal          0.9604   \n",
       "\n",
       "   fear  anger  anticipation     trust  surprise  positive  negative  sadness  \\\n",
       "0  0.25   0.25      0.000000  0.000000  0.000000  0.250000      0.25      0.0   \n",
       "1  0.00   0.00      0.000000  0.000000  0.000000  0.000000      0.00      0.0   \n",
       "2  0.00   0.00      0.000000  0.000000  0.000000  0.000000      0.00      0.0   \n",
       "3  0.00   0.00      0.142857  0.428571  0.142857  0.142857      0.00      0.0   \n",
       "4  0.00   0.00      0.250000  0.187500  0.062500  0.312500      0.00      0.0   \n",
       "\n",
       "   disgust       joy  \n",
       "0      0.0  0.000000  \n",
       "1      0.0  0.000000  \n",
       "2      0.0  0.000000  \n",
       "3      0.0  0.142857  \n",
       "4      0.0  0.187500  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get liwc with custom dictionary\n",
    "def count_liwc(df):\n",
    "    # load the liwc dictionary\n",
    "    liwcPath = 'C://Users//lvlip//Documents//BCSI Sem 6//FYP 4202//CSV//liwc_dic.dic'\n",
    "    # parse the dictionary\n",
    "    parse, category_names = liwc.load_token_parser(liwcPath)\n",
    "    # tokenize the tweets\n",
    "    text_tokens = [word_tokenize(x) for x in df['cleaned_tweet']]\n",
    "    \n",
    "    # count the frequency of each category\n",
    "    combine_liwc = []\n",
    "\n",
    "    for text in text_tokens:\n",
    "        word_counts = Counter(category for word in text for category in parse(word))\n",
    "        #print(word_counts.Counter())\n",
    "        combine_liwc.append(word_counts)\n",
    "    \n",
    "    # extract counter dictionary - previous result Counter({}), extract the {}\n",
    "    combine_dic = []\n",
    "\n",
    "    for i in range (len(combine_liwc)):\n",
    "        dic = {}\n",
    "        for k, v in combine_liwc[i].items():\n",
    "            dic[k] = v\n",
    "        #print(dic)\n",
    "        combine_dic.append(dic)\n",
    "        \n",
    "    # convert dictionary into dataframe\n",
    "    liwc_df = pd.DataFrame(combine_dic)\n",
    "    \n",
    "    # replace NaN with 0\n",
    "    liwc_df = liwc_df.fillna(0)\n",
    "    \n",
    "    # concat df with liwc\n",
    "    df = pd.concat([df, liwc_df], axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = count_liwc(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GSDMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic modeling\n",
    "def train_topic(df):\n",
    "    # tokenize tweets into words\n",
    "    docs = [word_tokenize(x) for x in df['cleaned_tweet']]\n",
    "    \n",
    "    # remove stopwords from docs\n",
    "    docs_cleaned = []\n",
    "    stop_words = gensim.parsing.preprocessing.STOPWORDS\n",
    "    \n",
    "    for doc in docs:\n",
    "        temp = []\n",
    "        for word in doc:\n",
    "            if word not in stop_words:\n",
    "                temp.append(word)\n",
    "        #print(temp)\n",
    "        docs_cleaned.append(temp)\n",
    "\n",
    "    # create dictionary of all words in docs_cleaned\n",
    "    dictionary = gensim.corpora.Dictionary(docs_cleaned)\n",
    "    \n",
    "    # find the length of dictionary\n",
    "    dic_length = len(dictionary)\n",
    "\n",
    "    # initialize GSDMM\n",
    "    np.random.seed(1000)\n",
    "    gsdmm = MovieGroupProcess(K=10, alpha=0.1, beta=0.1, n_iters=30)\n",
    "\n",
    "    # fit GSDMM model\n",
    "    gsdmm.fit(docs_cleaned, dic_length)\n",
    "    return gsdmm, docs_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In stage 0: transferred 8593 clusters with 10 clusters populated\n",
      "In stage 1: transferred 6089 clusters with 10 clusters populated\n",
      "In stage 2: transferred 5059 clusters with 10 clusters populated\n",
      "In stage 3: transferred 4319 clusters with 10 clusters populated\n",
      "In stage 4: transferred 3920 clusters with 10 clusters populated\n",
      "In stage 5: transferred 3647 clusters with 10 clusters populated\n",
      "In stage 6: transferred 3478 clusters with 10 clusters populated\n",
      "In stage 7: transferred 3324 clusters with 10 clusters populated\n",
      "In stage 8: transferred 3200 clusters with 10 clusters populated\n",
      "In stage 9: transferred 3226 clusters with 10 clusters populated\n",
      "In stage 10: transferred 3172 clusters with 10 clusters populated\n",
      "In stage 11: transferred 3148 clusters with 10 clusters populated\n",
      "In stage 12: transferred 3129 clusters with 10 clusters populated\n",
      "In stage 13: transferred 3079 clusters with 10 clusters populated\n",
      "In stage 14: transferred 3078 clusters with 10 clusters populated\n",
      "In stage 15: transferred 2890 clusters with 10 clusters populated\n",
      "In stage 16: transferred 2864 clusters with 10 clusters populated\n",
      "In stage 17: transferred 2785 clusters with 10 clusters populated\n",
      "In stage 18: transferred 2734 clusters with 10 clusters populated\n",
      "In stage 19: transferred 2676 clusters with 10 clusters populated\n",
      "In stage 20: transferred 2568 clusters with 10 clusters populated\n",
      "In stage 21: transferred 2540 clusters with 10 clusters populated\n",
      "In stage 22: transferred 2444 clusters with 10 clusters populated\n",
      "In stage 23: transferred 2342 clusters with 10 clusters populated\n",
      "In stage 24: transferred 2297 clusters with 10 clusters populated\n",
      "In stage 25: transferred 2157 clusters with 10 clusters populated\n",
      "In stage 26: transferred 2123 clusters with 10 clusters populated\n",
      "In stage 27: transferred 2137 clusters with 10 clusters populated\n",
      "In stage 28: transferred 1987 clusters with 10 clusters populated\n",
      "In stage 29: transferred 2054 clusters with 10 clusters populated\n"
     ]
    }
   ],
   "source": [
    "gsdmm, docs_cleaned = train_topic(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(gsdmm, open('gsdmm.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents per topic: [ 303  209 5376  850  136 1179  175  684  772  863]\n"
     ]
    }
   ],
   "source": [
    "# print number of documents per topic\n",
    "doc_count = np.array(gsdmm.cluster_doc_count)\n",
    "print('Number of documents per topic:', doc_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most important topics: [2 5 9 3 8 7 0 1 6 4]\n"
     ]
    }
   ],
   "source": [
    "# topics sorted by the number of document they are allocated to\n",
    "top_topic = doc_count.argsort()[-10:][::-1]\n",
    "print('Most important topics:', top_topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cluster 2 : [('feel', 1100), ('want', 962), ('like', 962), ('know', 643), ('life', 435), ('time', 422), ('people', 417), ('fuck', 393), ('day', 377), ('think', 365), ('bad', 350), ('hate', 348), ('thing', 334), ('try', 309), ('need', 279), ('good', 271), ('die', 234), ('love', 212), ('hurt', 211), ('tell', 208)]\n",
      "\n",
      "Cluster 5 : [('like', 131), ('know', 130), ('people', 128), ('want', 98), ('love', 94), ('think', 79), ('time', 68), ('thing', 66), ('way', 65), ('look', 59), ('good', 59), ('work', 58), ('need', 55), ('life', 52), ('friend', 52), ('use', 51), ('try', 47), ('day', 47), ('best', 43), ('help', 40)]\n",
      "\n",
      "Cluster 9 : [('time', 77), ('like', 73), ('good', 70), ('day', 67), ('year', 66), ('love', 54), ('think', 48), ('best', 48), ('christmas', 46), ('game', 44), ('happy', 42), ('great', 38), ('morning', 37), ('wait', 36), ('new', 36), ('work', 35), ('man', 35), ('laugh', 34), ('play', 34), ('movie', 33)]\n",
      "\n",
      "Cluster 3 : [('good', 75), ('time', 67), ('team', 65), ('best', 61), ('year', 59), ('win', 55), ('project', 52), ('game', 44), ('people', 44), ('like', 43), ('big', 38), ('great', 37), ('think', 36), ('know', 36), ('play', 35), ('day', 35), ('thing', 32), ('let', 32), ('need', 32), ('player', 31)]\n",
      "\n",
      "Cluster 8 : [('like', 75), ('day', 61), ('night', 60), ('time', 56), ('sleep', 53), ('look', 52), ('love', 43), ('think', 40), ('want', 40), ('know', 38), ('feel', 36), ('good', 34), ('eat', 33), ('laugh', 31), ('use', 29), ('need', 28), ('hour', 28), ('bed', 26), ('way', 25), ('morning', 25)]\n",
      "\n",
      "Cluster 7 : [('people', 66), ('know', 55), ('need', 53), ('time', 47), ('day', 42), ('think', 40), ('work', 39), ('like', 36), ('way', 36), ('vaccine', 35), ('covid', 34), ('tell', 30), ('new', 28), ('pay', 28), ('right', 27), ('good', 27), ('year', 26), ('use', 25), ('want', 25), ('look', 25)]\n",
      "\n",
      "Cluster 0 : [('good', 31), ('like', 31), ('love', 27), ('thing', 22), ('people', 22), ('time', 20), ('year', 16), ('want', 16), ('think', 16), ('man', 14), ('read', 14), ('talk', 13), ('look', 13), ('world', 13), ('movie', 13), ('come', 12), ('book', 12), ('know', 10), ('power', 10), ('write', 10)]\n",
      "\n",
      "Cluster 1 : [('come', 13), ('way', 12), ('kick', 10), ('problem', 9), ('urban', 9), ('snow', 9), ('laugh', 8), ('c', 8), ('time', 7), ('think', 7), ('year', 7), ('want', 7), ('best', 7), ('water', 7), ('old', 7), ('create', 7), ('n', 7), ('grinch', 7), ('king', 7), ('good', 6)]\n",
      "\n",
      "Cluster 6 : [('people', 18), ('think', 12), ('try', 12), ('truth', 11), ('lie', 11), ('look', 10), ('men', 10), ('la', 10), ('word', 9), ('right', 8), ('purpose', 8), ('want', 7), ('time', 7), ('wear', 7), ('support', 7), ('data', 7), ('police', 7), ('way', 6), ('life', 6), ('hand', 6)]\n",
      "\n",
      "Cluster 4 : [('like', 20), ('man', 9), ('sound', 8), ('think', 7), ('f', 7), ('fight', 6), ('completely', 6), ('rabbit', 6), ('chicken', 6), ('right', 5), ('god', 5), ('come', 5), ('love', 5), ('color', 5), ('perfect', 5), ('real', 5), ('ck', 5), ('melt', 5), ('version', 5), ('soup', 5)]\n"
     ]
    }
   ],
   "source": [
    "# get top words per topic\n",
    "for topic in top_topic:\n",
    "    sort_dicts = sorted(gsdmm.cluster_word_distribution[topic].items(), key=lambda k: k[1], reverse=True)[:20]\n",
    "    print(\"\\nCluster %s : %s\"%(topic, sort_dicts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract topic for each tweet\n",
    "def extract_topic(df, gsdmm, docs_cleaned, top_topic):\n",
    "    topic = []\n",
    "    prob = [gsdmm.choose_best_label(x) for x in docs_cleaned]\n",
    "\n",
    "    threshold = 0.3\n",
    "    \n",
    "    # confirm the topic only when the prob is >= 0.3, else assign 10 as other topic\n",
    "    for data in prob:\n",
    "        if data[1] >= threshold:\n",
    "            topic.append(top_topic[data[0]])\n",
    "        else:\n",
    "            topic.append(10)\n",
    "        \n",
    "    # add the topic column to df\n",
    "    df['topic'] = topic\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = extract_topic(df, gsdmm, docs_cleaned, top_topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>username</th>\n",
       "      <th>datetime</th>\n",
       "      <th>tweet</th>\n",
       "      <th>cleaned_tweet</th>\n",
       "      <th>target</th>\n",
       "      <th>tweet_compound</th>\n",
       "      <th>fear</th>\n",
       "      <th>anger</th>\n",
       "      <th>anticipation</th>\n",
       "      <th>trust</th>\n",
       "      <th>...</th>\n",
       "      <th>primarysupport</th>\n",
       "      <th>troubleconcentrate</th>\n",
       "      <th>worthlessness/guilt</th>\n",
       "      <th>housing</th>\n",
       "      <th>disturbedsleep</th>\n",
       "      <th>occupational</th>\n",
       "      <th>fatigue/lossenergy</th>\n",
       "      <th>weight/appetite</th>\n",
       "      <th>agitation/retardation</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>robertevans97</td>\n",
       "      <td>16/12/21 14:27:14</td>\n",
       "      <td>@arielhelwani @DustinPoirier @NateDiaz209 Big ...</td>\n",
       "      <td>big fight dustin want prove point nate</td>\n",
       "      <td>normal</td>\n",
       "      <td>-0.3182</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>flintmorris</td>\n",
       "      <td>16/12/21 14:27:14</td>\n",
       "      <td>@ItsRyanGonzales @100Thieves @RabidDoh @thundo...</td>\n",
       "      <td>they choose one best naruto opening song big</td>\n",
       "      <td>normal</td>\n",
       "      <td>0.6369</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>chardinite</td>\n",
       "      <td>16/12/21 14:27:14</td>\n",
       "      <td>@Tinkzorg Gen Z can't wield the mandate of hea...</td>\n",
       "      <td>gen z not wield mandate heaven confirm</td>\n",
       "      <td>normal</td>\n",
       "      <td>0.5106</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ghobubo</td>\n",
       "      <td>16/12/21 14:27:14</td>\n",
       "      <td>@JortsTheCat has his own account and 20K follo...</td>\n",
       "      <td>his account follower sweet potato</td>\n",
       "      <td>normal</td>\n",
       "      <td>0.4588</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mdarahimkhan2</td>\n",
       "      <td>16/12/21 14:27:14</td>\n",
       "      <td>@BitKeepOS @NEARProtocol This is a really grea...</td>\n",
       "      <td>really great excellent project thank you oppor...</td>\n",
       "      <td>normal</td>\n",
       "      <td>0.9604</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        username           datetime  \\\n",
       "0  robertevans97  16/12/21 14:27:14   \n",
       "1    flintmorris  16/12/21 14:27:14   \n",
       "2     chardinite  16/12/21 14:27:14   \n",
       "3        ghobubo  16/12/21 14:27:14   \n",
       "4  mdarahimkhan2  16/12/21 14:27:14   \n",
       "\n",
       "                                               tweet  \\\n",
       "0  @arielhelwani @DustinPoirier @NateDiaz209 Big ...   \n",
       "1  @ItsRyanGonzales @100Thieves @RabidDoh @thundo...   \n",
       "2  @Tinkzorg Gen Z can't wield the mandate of hea...   \n",
       "3  @JortsTheCat has his own account and 20K follo...   \n",
       "4  @BitKeepOS @NEARProtocol This is a really grea...   \n",
       "\n",
       "                                       cleaned_tweet  target  tweet_compound  \\\n",
       "0             big fight dustin want prove point nate  normal         -0.3182   \n",
       "1       they choose one best naruto opening song big  normal          0.6369   \n",
       "2             gen z not wield mandate heaven confirm  normal          0.5106   \n",
       "3                  his account follower sweet potato  normal          0.4588   \n",
       "4  really great excellent project thank you oppor...  normal          0.9604   \n",
       "\n",
       "   fear  anger  anticipation     trust  ...  primarysupport  \\\n",
       "0  0.25   0.25      0.000000  0.000000  ...             0.0   \n",
       "1  0.00   0.00      0.000000  0.000000  ...             0.0   \n",
       "2  0.00   0.00      0.000000  0.000000  ...             0.0   \n",
       "3  0.00   0.00      0.142857  0.428571  ...             0.0   \n",
       "4  0.00   0.00      0.250000  0.187500  ...             0.0   \n",
       "\n",
       "   troubleconcentrate  worthlessness/guilt  housing  disturbedsleep  \\\n",
       "0                 0.0                  0.0      0.0             0.0   \n",
       "1                 0.0                  0.0      0.0             0.0   \n",
       "2                 0.0                  0.0      0.0             0.0   \n",
       "3                 0.0                  0.0      0.0             0.0   \n",
       "4                 0.0                  0.0      0.0             0.0   \n",
       "\n",
       "   occupational  fatigue/lossenergy  weight/appetite  agitation/retardation  \\\n",
       "0           0.0                 0.0              0.0                    0.0   \n",
       "1           0.0                 0.0              0.0                    0.0   \n",
       "2           0.0                 0.0              0.0                    0.0   \n",
       "3           0.0                 0.0              0.0                    0.0   \n",
       "4           0.0                 0.0              0.0                    0.0   \n",
       "\n",
       "   topic  \n",
       "0      1  \n",
       "1      4  \n",
       "2      5  \n",
       "3      7  \n",
       "4      3  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 611,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10547, 29)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('depression_detection.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
